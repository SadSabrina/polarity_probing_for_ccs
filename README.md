# **"Polarity-Aware Probing for Quantifying Latent Alignment in Language Models."**

Source code for the paper ["Polarity-Aware Probing for Quantifying Latent Alignment in Language Models"]().

![img](https://raw.githubusercontent.com/SadSabrina/ccs_2/refs/heads/main/teaser/ccs_teaser.png)

## **Abstract**

Recent progress in unsupervised probing methods, notably Contrastâ€‘Consistent Search (CCS) (Stoehr et al. 2024), has enabled the extraction of latent beliefs in language models without relying on token-level outputs. As these probes offer lightweight diagnostic tools with low alignment tax, a central question arises: *can they effectively assess model alignment?* We investigate this by examining CCS's sensitivity to harmful vs. safe statements and introducing Polarityâ€‘Aware CCS (PAâ€‘CCS), which evaluates whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics â€” **Polarâ€‘Consistency** and **Contradiction Index** â€” to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate **two main and one control datasets** containing matched harmful-safe sentence pairs formulated by different methods (concurrent and antagonistic statements), and apply PA-CCS to **16 language models**. Our results demonstrate that PAâ€‘CCS reveals both architectural and layer-specific differences in the encoding of latent harmful knowledge. Interestingly, replacing the negation token with a meaningless marker degrades the PAâ€‘CCS scores of models with aligned representations. In contrast, models lacking robust internal calibration do not show this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and call on the community to incorporate structural robustness checks into interpretability benchmarks.

---

## Datasets

The paper includes the release of new datasets containing contrasting pairs of "harmful/benign" statements.

### Dataset Overview

| Dataset | Link | Total Samples | Unique Observations | Harm-Safe Pairs | Purpose |
|---------|------|--------------|---------------------|-----------------|---------|
| Mixed dataset | [ðŸ¤— Huggingface](https://huggingface.co/datasets/SabrinaSadiekh/mixed_hate_dataset) | 1244 | 1244 | 622 | Tests whether CCS can distinguish harmful from safe beliefs in realistic, naturally varied formulations |
| Not dataset | [ðŸ¤— Huggingface](https://huggingface.co/datasets/SabrinaSadiekh/not_hate_dataset) | 1250 | 1250 | 625 | Direct evaluation of how the model handles polarity flips in tightly aligned sentences |

### Dataset Construction Details

<table>
<thead>
<tr>
<th>Dataset</th>
<th>Method/Category</th>
<th>Description</th>
<th>Proportion</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="4"><strong>"Mixed" Dataset</strong></td>
</tr>
<tr>
<td></td>
<td>Concurrent-based</td>
<td>Harmful and safe statements differ by rephrasing, while preserving semantic opposition</td>
<td>74.7%</td>
</tr>
<tr>
<td></td>
<td>Negation-based</td>
<td>One statement is the syntactic negation of the other</td>
<td>26.3%</td>
</tr>
<tr>
<td colspan="4"><strong>"Not" Dataset</strong></td>
</tr>
<tr>
<td></td>
<td>Harmful version</td>
<td>Contains "not" in 51% of pairs where $x^{\text{harm}} = \texttt{not}(x^{\text{safe}})$ or vice versa</td>
<td>51%</td>
</tr>
<tr>
<td></td>
<td>Safe version</td>
<td>Controlled negation setting with tightly aligned sentences</td>
<td>49%</td>
</tr>
</tbody>
</table>

---

## **How to use this repository?**

The primary goal of this repository is to ensure the reproducibility of the results of "Polarity-Aware Probing for Quantifying Latent Alignment in Language Models." However, we encourage you to use our results not only for correctness analysis but also for your own research. You can:

1. Explore the behavior of metrics on your own models and analyze internal polarity compared to the model output.
2. Use datasets balanced by categories and utterance types in your experiments. Datasets are available for download in the HF ([mixed dataset](https://huggingface.co/datasets/SabrinaSadiekh/mixed_hate_dataset), [not dataset](https://huggingface.co/datasets/SabrinaSadiekh/not_hate_dataset)).
3. Use visualizations in presentations and lectures on the geometric organization of data within a model.

---
---

## **Files and folders**

TO DO:

- [ ] make cleaner
- [ ] make readme for datasets on HF

**Repo structure:**

```bash
code/
  â”œâ”€â”€ ccs.py
  â”œâ”€â”€ extract.py
  â”œâ”€â”€ extract_llama.py
  â”œâ”€â”€ format_results.py

data/
  â”œâ”€â”€ raw/
  â”‚   â”œâ”€â”€ mixed_dataset.csv
  â”‚   â””â”€â”€ not_dataset.csv
  â”œâ”€â”€ yes_no/
  â”‚   â”œâ”€â”€ mixed_dataset_no.csv
  â”‚   â”œâ”€â”€ mixed_dataset_yes.csv
  â”‚   â”œâ”€â”€ not_dataset_no.csv
  â”‚   â””â”€â”€ not_dataset_yes.csv

notebooks/
  â”œâ”€â”€ ccs_deberta_pretr.ipynb
  â””â”€â”€ ccs_Meta-Llama-3-8B-Instruct.ipynb

```

### **Description**

#### `code/` â€” Source Code for PA-CCS Evaluation

This directory contains all the necessary scripts for reproducing the experiments described in our paper on **Polarity-Aware Probing for Quantifying Latent Alignment in Language Models** for analyzing internal representations in language models.

The code is organized to cleanly separate:

- Feature extraction (`extract.py`, `extract_llama.py`)
- Probing & analysis (`ccs.py`)
- Evaluation & reporting (`format_results.py`)

##### Files

##### `ccs.py`

Base implementation of the **Contrast Consistent Search (CCS)** method for linear probing of model representations.

- Implements the linear probing training and evaluation procedure.
- Used to compute CCS directions and evaluate the empirical separation accuracy (ESA) and introduced metrics: polar consistency (PC), and contradiction index (CI).

##### `extract.py`

Script for extracting hidden states from encoder, decoder and encoder-decoder based models.

##### `extract_llama.py`

Script for extracting representations from LLaMA â€” just `extract.py` adapted for MetaLlama8b models.

##### `format_results.py`

Post-processing utility that:

- Aggregates layerwise metric results (ESA, PC, CI).
- Computes group statistics (means, std, confidence intervals).
- Formats results for plotting or tabular reporting.

##### Usage Notes

- All scripts assume that the input data is already formatted as sentence pairs with polarity labels.
- The dataset with pairs should be organized as follows. From $0, 1, 2 .... N$ sentences, the first $\frac{N}{2}$ harm (or safe) and the next $\frac{N}{2}$ safe (or harm). For 0, the pair index is $\frac{N}{2}$, for 1, the pair index is $\frac{N}{2} + 1$ and so on.
- The model interface is built on top of HuggingFace converters.

#### **`notebooks/` â€” PA-CCS Evaluation Notebooks**

This folder contains Jupyter notebooks for running and plotting **Polarity-Aware CCS (PA-CCS)** on various language models. The notebooks are intended to reproduce key experiments and visualize model alignment behavior.

##### Files

- **`ccs_deberta_pretr.ipynb`**
  Runs CCS and PA-CCS on **DeBERTa-large-FT** (`Elron/deberta-v3-large-hate`). Includes:

- Extract hidden representations
- PA-CCC training
- Calculate and format ESA/PC/CI metrics
- Visual diagnostics of metrics and separations

- **`ccs_Meta-Llama-3-8B-Instruct.ipynb`**
  Applies PA-CCS to **Meta-LLaMA-3 8B Instruct** using reformulated harmful-safe statement pairs. Includes:

- Extract hidden representations
- PA-CCC training
- Calculate and format ESA/PC/CI metrics
- Visual diagnostics of metrics and separations

#### **`data/` â€” datasets introduced in paper**

#### Requirements

- Python â‰¥ 3.9
- `transformers`, `torch`, `datasets`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn`, `pandas`, `sentencepiece
`, `accelerate`, `tqdm`
- For LLaMA models: use HF-compatible checkpoints (with proper access)
